---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# SimHelpers

<!-- badges: start -->
<!-- badges: end -->

The goal of SimHelpers is to help with running simulation studies. It calculates performance criteria measures, Monte Carlo Standard Errors...(under development).

## Installation

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("meghapsimatrix/SimHelpers")
```

# Introduction to Simulation Studies and Monte Carlo Standard Errors

Simulations are experiments to study performance of statistical methods under known data-generating conditions (Morris, White and Crowther, 2018). Methodologists examine questions like: (1) how does ordinary least squares (OLS) regression perform if errors are heteroskedastic? (2) how does the presence of missing data affect treatment effect estimates from a propensity score analysis? To answer such questions, we conduct experiments by simulating thousands of datasets from pseudo-random sampling (Morris et al., 2018). We generate datasets under conditions where the errors are heteroskedastic or there is missing data present following Missing at Random (MAR) mechanism. Then, we apply statistical methods, like OLS, to each of the datasets and extract measures like effect estimate, p values, confidence intervals. We then analyze the methods using performance criteria like bias, type 1 error rate etc. described below. 

When examining performance measures, we need to also consider the error in the estimate of the performance measures due to the fact that we generate a finite number of samples. The error is quantified in the form of Monte Carlo standard error (MCSE). This package provides a function that calculates various performance measures and associated MCSE. Below, I describe the performance criteria covered in this package and the formula for calculating the performance measures and the MCSE (Morris et al., 2018).

The formula notations and explanations of the performance measures follow notes from Dr. James Pustejovsky's [Data Analysis, Simulation and Programming in R course (Spring, 2019)](https://www.jepusto.com/teaching/daspir/).

# Performance Criteria and MCSE

## Absolute Criteria

Bias characterizes whether the estimate on average lies above or below the true parameter. 
Variance characterizes the precision of the estimates - how much does each estimate deviates from the average of the estimates? Note that variance is inverse of precision - the higher the variance, the lower the precision. Mean square error and root mean square error characterizes the accuracy of the estimates. It measures how far off on average are the estimates from the true parameter. Absolute criteria will be in the scale of the estimates.

### Example

We use the `welch_res` dataset included in the package. It contains the results from an example simulation study comparing Welch t-test to normal t-test. The code used to generate the data and results can be found [here](https://github.com/meghapsimatrix/SimHelpers/blob/master/data-raw/welch_res_dat.R). Below, we calculate the absolute performance criteria for the estimate of the mean difference. We present the results by method and mean difference (conditions which were varied to generate the `welch_res` data). The `calc_abs` function is designed to work with the `dplyr` `tidyeval` workflow. The first argument, `res_dat`, is a dataset containing the results from a simulation study. The second argument, `estimates`, is the name of the column containing the estimates like mean difference or regression coefficients. The third argument, `true_param`, is the name of the column containing the true parameters. The fourth argument `perfm_criteria` lets the user specify which criteria to evaluate. In the example below, we ask for all the available criteria. 

We use `dplyr` syntax to group by two conditions that were varied to generate the simulation data: method and mean differences. We provide examples using `do()` and `group_modify()` to run `calc_abs()` on results for each combination of the conditions. 

```{r example, message = FALSE, warning = FALSE}
library(SimHelpers)
library(tidyverse)
library(knitr)

# using do()
welch_res %>%
  group_by(method, mean_diff) %>%
  do(calc_abs(.,  estimates = est, true_param = mean_diff)) %>%
  kable()
```

```{r}
# using group_modify()
welch_res %>%
  mutate(params = mean_diff) %>%
  group_by(method, mean_diff) %>%
  group_modify(~ calc_abs(.x,  estimates = est, true_param = params)) %>%
  kable()
```



## Relative Criteria

Relative criteria can be useful to look at if multiple true parameter values are used to generate the data to run the simulation and if performance measure changes according to the true value. It can be only used when $|\theta| > 0$ as we cannot divide by $0$ (Morris et al., 2018). 

### Example 

Below, we calculate the relative criteria for the mean difference estimates. Note that when mean difference is 0, the relative measures cannot be calculated. The function returns `NA` values for conditions where the mean difference is 0. The syntax is similar to the one that we used earlier for `calc_abs`.


```{r}
# using do()
welch_res %>%
  group_by(method, mean_diff) %>%
  do(calc_relative(., estimates = est, true_param = mean_diff)) %>%
  kable()
```

```{r}
# using group_modify()
welch_res %>%
  mutate(params = mean_diff) %>%
  group_by(method, mean_diff) %>%
  group_modify(~ calc_relative(.x,  estimates = est, true_param = params)) %>%
  kable()
```



## Hypothesis Testing and Confidence Intervals

When doing hypothesis tests, we are often interested in whether Type 1 error rate is controlled and whether the test has enough power to detect the effect of interest. Rejection rate captures the proportion of times the p value is below a specified $\alpha$ level, the proportion of times we can reject the null hypothesis. When the specified effect is 0, we can examine Type 1 error rates and when the magnitude of the effect is greater than 0, we can examine power. We are also interested in confidence intervals, proportion of intervals that contain the true parameter, and interval width, which is an indicator for the precision of the estimate.

### Example

Below we calculate the rejection rate performance criteria for the hypothesis tests done using t-test. The `calc_rr` function takes in a dataset as the first argument. The second argument, `p_values` requires a column containing p values.  

```{r}
# using do()
welch_res %>%
  group_by(method, mean_diff) %>%
  do(calc_rr(., p_values = p_val)) %>%
  kable()
```

```{r}
# using group_modify()
welch_res %>%
  group_by(method, mean_diff) %>%
  group_modify(~ calc_rr(.x, p_values = p_val)) %>%
  kable()
```



Below we calculate the coverage and width performance criteria for the confidence intervals for the estimate of the mean difference. The `calc_coverage` function takes in a result data as the first argument. The second and third arguments, `lower_bound` and `upper_bound` take in columns that contain the lower and upper bound estimates of the confidence interval. The `true_param` argument takes in a column containing the true parameters. Like `calc_abs` and `calc_relative`, `calc_coverage` also has an argument `perfm_criteria` where the user can specify which criteria to evaluate. 

```{r}
# using do()
welch_res %>%
  group_by(method, mean_diff) %>%
  do(calc_coverage(., lower_bound = lower_bound, upper_bound = upper_bound, true_param = mean_diff)) %>%
  kable()
```

```{r}
# using group_modify()
welch_res %>%
  mutate(params = mean_diff) %>%
  group_by(method, mean_diff) %>%
  group_modify(~ calc_coverage(.x, lower_bound = lower_bound, upper_bound = upper_bound, true_param = params)) %>%
  kable()
```



# Jacknife MCSE for variance estimates

UNDER DEVELOPMENT :) 

```{r}
calc_jacknife(res_dat = alpha_res, estimates = Var_A, true_param = true_param) %>%
  kable()
```

