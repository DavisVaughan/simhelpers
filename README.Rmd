---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# SimHelpers

<!-- badges: start -->
<!-- badges: end -->

The goal of SimHelpers is to help with running simulation studies. It calculates performance criteria measures, Monte Carlo Standard Errors...(under development).

## Installation

You can install the development version from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("meghapsimatrix/SimHelpers")
```
## Example

This is a basic example which shows you how to solve a common problem. WE use the `welch_res` dataset included in the package. Here we are calculating the absolute performance criteria for the estimate of the mean difference. 


```{r example, message = FALSE, warning = FALSE}
library(SimHelpers)
library(tidyverse)
library(knitr)


welch_res %>%
  group_by(method, mean_diff) %>%
  do(calc_abs(., true_param = .$mean_diff[1], K = .$iterations[1])) %>%
  kable()
```

Below, we calculate the relative criteria for the mean difference estimates. Note that when mean difference is 0, the relative measures cannot be calculated. 

```{r}
welch_res %>%
  group_by(method, mean_diff) %>%
  do(calc_relative(., true_param = .$mean_diff[1], K = .$iterations[1])) %>%
  kable()

```


Below we calculate the rejection rate performance criteria for the hypothesis tests done using t-test.  

```{r}
welch_res %>%
  group_by(method, mean_diff) %>%
  do(calc_rr(., K = .$iterations[1])) %>%
  kable()
```


Below we calculate the coverage and width performance criteria for the confidence intervals for the estimate of the mean difference.

```{r}
welch_res %>%
  group_by(method, mean_diff) %>%
  do(calc_coverage(., true_param = .$mean_diff[1], K = .$iterations[1])) %>%
  kable()
```


