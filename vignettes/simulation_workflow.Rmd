---
title: "Simulation Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{simulation_workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{dplyr}
  %\VignetteDepends{tibble}
  %\VignetteDepends{knitr}
  %\VignetteDepends{kableExtra}
  %\VignetteDepends{purrr}
  %\VignetteDepends{tidyr}
  %\VignetteDepends{DiagrammeR}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```



Simulation studies generally follow the following workflow (Morris, White & Crowther, 2018): 

1. Create an experimental design
2. Generate a dataset
3. Run statistical methods on the dataset
4. Measure performance criteria and calculate MCSE
5. Run steps 1-3 thousands of times for each of the experimental design conditions
6. Evaluate the overall results

The following flowchart depicts the workflow. The chart is created using grViz() from `DiagrammeR` package.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(DiagrammeR)

grViz("digraph flowchart {
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = '@@1']
      tab2 [label = '@@2']
      tab3 [label = '@@3']
      tab4 [label = '@@4']
      tab5 [label = '@@5']

      # edge definitions with the node IDs
      tab1 -> tab2 -> tab3 -> tab4 -> tab5;
      }

      [1]: 'Experimental Design'
      [2]: 'Data Generating Model'
      [3]: 'Estimation Methods'
      [4]: 'Performance Summaries'
      [5]: 'Results'
      ")
```

The explanation of the workflow in this vignette follow notes from Dr. James Pustejovsky's [Data Analysis, Simulation and Programming in R course (Spring, 2019)](https://www.jepusto.com/teaching/daspir/).

## Libraries

```{r, warning = FALSE, message = FALSE}
library(SimHelpers)
library(dplyr)
library(tibble)
library(purrr)
library(tidyr)
library(knitr)
library(kableExtra)
```


## Experimental Design Initial

Before we begin working on the simulation study, we should decide what model and design parameters we want to vary. Parameters can include sample size, proportion of missing data etc. 


## Generating Dataset

The data generating function below takes in model parameters. For example, we can generate data varying the sample size or level of heteroskedasticity or the amount of missingness. Below is a skeleton of the data generating function. The arguments are any data generating parameters that we would want to vary. 

```{r, eval = FALSE}
generate_dat <- function(model_params) {

  return(dat)
}
```

Below is an example where we generate random normal data for two groups. The function takes in two arguments: `n`, indicating sample size per group assuming equal sample size, and `mean_diff`, indicating the mean difference. 

```{r}
generate_dat <- function(n, mean_diff){

  dat <- tibble(group_1 = rnorm(n = n, 0, 1),
                group_2 = rnorm(n = n, mean_diff, 2))

  return(dat)

}
```


## Estimation Methods

Here, we run some statistical methods to estimate a test statistic, regression coefficients, p-values, or confidence intervals. The function takes in the data and any design parameters. 

```{r, eval = FALSE}
estimate <- function(dat, design_params) {

  return(results)
}
```

Below is an example function that runs a t-test on a simulated dataset. The function runs a normal t-test, which assumes homogeneity of variance and a Welch t-test, which does not assume that the population variances of the outcome for the two groups are equal. The function returns a tibble containing the method, mean difference estimate, p-value, and upper and lower bounds of the confidence interval. We could use the `t.test()` function to extract everything we need but we are calculating the t-statistic using sample statistics just for fun.

```{r}
estimate <- function(dat){

  # calculate summary stats
  est <- mean(dat$group_1) - mean(dat$group_2)
  n1 <- n2 <- nrow(dat)
  var_1 <- var(dat$group_1)
  var_2 <- var(dat$group_2)

  # normal t-test
  df <- n1 + n2 - 2
  sp_sq <- ((n1-1) * var_1 + (n2 - 1) * var_2) / df
  vd <- sp_sq * (1 / n1 + 1 / n2)



  # welch t-test
  dfw <- (var_1 / n1 + var_2 / n2)^2 / (((1 / (n1 - 1)) * (var_1 / n1)^2) + ((1 / (n2 - 1)) * (var_2 / n2)^2))
  vdw <- var_1 / n1 + var_2 / n2


  #t and pvalue
  calc_t <- function(est, vd, df, method){

    se <- sqrt(vd)
    t <- est / se
    p_val <-  2 * pt(-abs(t), df = df)
    ci <- est + c(-1, 1) * qt(.975, df = df) * se


    return(tibble(method = method, est = est, p_val = p_val, lower_bound = ci[1], upper_bound = ci[2]))
  }

  results <- bind_rows(calc_t(est = est, vd = vd, df = df, method = "t-test"),
                   calc_t(est = est, vd = vdw, df = dfw, method = "Welch t-test"))


  return(results)

}

```


## Performance Summaries 

In this step, we create a function to calculate performance measures based on the results that we extracted from the estimation step. The function below takes in the results, and any model parameters. 

```{r, eval = FALSE}
calc_performance <- function(results, model_params) {

  return(performance_measures)
}
```

The function below fills in the `calc_performance()`. We use the `calc_rr()` function in the `SimHelpers` package to calculate rejection rates. 

```{r}
calc_performance <- function(results) {
  
  performance_measures <- results %>%
    group_by(method) %>%
    do(calc_rr(., p_values = p_val))

  return(performance_measures)
}

```

## Simulation Driver 

The following code chunk sets up the simulation driver. The arguments specify the number of iterations for the simulation and any parameters that need to be specified to run the data generating and estimating functions. The function then calculates the performance measures and returns the data with the results. 

```{r, eval = FALSE}
run_sim <- function(iterations, model_params, design_params, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)

  results <-
    rerun(iterations, {
      dat <- generate_dat(model_params)
      estimate(dat, design_params)
    }) %>%
    bind_rows()

  calc_performance(results, model_params)
}
```

Below is the driver for our running example of a simulation study: 

```{r}
run_sim <- function(iterations, n, mean_diff, seed = NULL) {
  if (!is.null(seed)) set.seed(seed)

  results <-
    rerun(iterations, {
      dat <- generate_dat(n, mean_diff)
      estimate(dat)
    }) %>%
    bind_rows()
  
  calc_performance(results)

}

```


## Experimental Design Revisit

Now that we have all our functions in order, we can specify the exact factors we want to manipulate in the study. The following code chunk creates a list of design factors and uses the [`cross_df()`](https://purrr.tidyverse.org/reference/cross.html) function from [`purrr`](https://purrr.tidyverse.org/) package that creates every combination of the factor levels. 


```{r, eval = FALSE}
set.seed(20150316) # change this seed value!

# now express the simulation parameters as vectors/lists

design_factors <- list(factor1 = , factor2 = , ...) # combine into a design set

params <-
  cross_df(design_factors) %>%
  mutate(
    iterations = 1000,  # change this to how many ever iterations  
    seed = round(runif(1) * 2^30) + 1:n()
  )

# All look right?
lengths(design_factors)
nrow(params)
head(params)
```

The code below specifies two design factors: `n`, which specifies the sample size per group assuming equal sample size and `mean_diff`, which denotes the mean difference between two groups on an outcome. These are the between simulation factors. The within-simulation factor is the t-test method with one assuming equal variance and one not assuming equal variance. 

```{r}
set.seed(20200110)

# now express the simulation parameters as vectors/lists

design_factors <- list(
  n = c(50, 100),
  mean_diff = c(0, .5, 1, 2)
)

params <-
  cross_df(design_factors) %>%
  mutate(
    iterations = 1000,
    seed = round(runif(1) * 2^30) + 1:n()
  )


# All look right?
lengths(design_factors)
nrow(params)
head(params)

```



## Running the Simulation in Serial

Here we run the simulation using `purrr` package serial workflow:

```{r}
system.time(
  results <- 
    params %>%
    mutate(
      res = pmap(., .f = run_sim)
    ) %>%
    unnest(cols = res)
)

results %>%
  kable()
```

## Running the Simulation in Parallel

Below we use the [`future`](https://rstudio.github.io/promises/articles/futures.html) and the [`furrr`](https://davisvaughan.github.io/furrr/) package to run the simulation in parallel. These packages are designed to work with functions from the `purrr` package. The line below gets a cluster set up on your computer or network. For more complicated network setups, please see the [documentation for the future package](https://cran.r-project.org/web/packages/future/vignettes/future-1-overview.html). 

```{r, eval = FALSE}
plan(multisession)
```

Once the cluster is configured, we can just replace [`pmap()`](https://purrr.tidyverse.org/reference/map2.html) from `purrr` with [`future_pmap()`](https://davisvaughan.github.io/furrr/) to run the simulation in parallel. 

```{r}
library(future)
library(furrr)

plan(multisession) # choose an appropriate plan from the future package

system.time(
  results <-
    params %>%
    mutate(res = future_pmap(., .f = run_sim)) %>%
    unnest(cols = res)
)

results %>%
  kable()

```


In the `SimHelpers` package, we have a function `evaluate_by_row()` that implements the `furrr` workflow: 

```{r}
plan(multisession)
evaluate_by_row(params, run_sim) %>%
  kable()
```



# Example from SimHelpers

The `create_skeleton()` from our `SimHelpers` function will take in an argument specifying the name of the R file which will contain the skeleton of functions required to run a simulation study. 

```{r setup, eval = FALSE}
create_skeleton("Frane_replication")
```


# References 

Bengtsson, H. (2020). future: Unified Parallel and Distributed
  Processing in R for Everyone. R package version 1.16.0.
  https://CRAN.R-project.org/package=future
  
Iannone, R. (2020). DiagrammeR: Graph/Network Visualization. R package
  version 1.0.5. https://CRAN.R-project.org/package=DiagrammeR

Morris, T. P., White, I. R., & Crowther, M. J. (2019). Using simulation studies to evaluate statistical methods. Statistics in medicine, 38(11), 2074-2102.

Vaughan D., & Dancho, M. (2018). furrr: Apply Mapping Functions in
  Parallel using Futures. R package version 0.1.0.
  https://CRAN.R-project.org/package=furrr

Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source
  Software, 4(43), 1686, https://doi.org/10.21105/joss.01686
