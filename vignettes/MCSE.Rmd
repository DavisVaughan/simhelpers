---
title: "MCSE"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MCSE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{dplyr}
  %\VignetteDepends{tibble}
  %\VignetteDepends{knitr}
  %\VignetteDepends{kableExtra}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction to Simulation Studies and Monte Carlo Standard Errors

Simulations are experiments to study the performance of statistical methods under known data-generating conditions (Morris, White & Crowther, 2018). Methodologists examine questions like: (1) how does ordinary least squares (OLS) regression perform if errors are heteroskedastic? (2) how does the presence of missing data affect treatment effect estimates from a propensity score analysis? To answer such questions, we conduct experiments by simulating thousands of datasets from pseudo-random sampling (Morris et al., 2018). We generate datasets under known conditions. For example, we can generate data where the errors are heteroskedastic or there is missing data present following Missing at Random (MAR) mechanism. Then, we apply statistical methods, like OLS and propensity score estimation using logistic regression, to each of the datasets and extract measures like mean difference, regression coefficients, p-values, and confidence intervals. We then analyze the methods using performance criteria measures like bias, Type 1 error rate etc.

When examining performance measures, we need to also consider the error in the estimate of the performance measures due to the fact that we generate a finite number of samples. The error is quantified in the form of Monte Carlo standard error (MCSE). 

The goal of `SimHelpers` is to assist in running simulation studies. This package provides a set of functions that calculates various performance measures and associated MCSE. This vignette focuses on the set of functions. Below, we explain three major categories of performance criteria: absolute, relative and hypothesis testing. We provide formulas used to calculate the performance measures and MCSE and demonstrate how to use the functions in the `SimHelpers` package. 

The formula notations and explanations of the performance measures follow notes from Dr. James Pustejovsky's [Data Analysis, Simulation and Programming in R course (Spring, 2019)](https://www.jepusto.com/teaching/daspir/).


# Performance Criteria and MCSE

## Absolute Criteria

Bias characterizes whether the estimate on average lies above or below the true parameter. 
Variance characterizes the precision of the estimates. It characterizes the deviation of each estimate from the average of the estimates. Note that variance is the inverse of precision. Therefore, the higher the variance, the lower the precision. Mean squared error (MSE) and root mean squared error (RMSE) characterize the accuracy of the estimates. MSE and RMSE measure how far off on average the estimates are from the true parameter. Absolute criteria are in the same scale as the estimates. MSE is in squared deviation scale and RMSE is in the scale of the estimates.

Let $T$ denote an estimator for a parameter $\theta$. From running a simulation study, we obtain $K$ iterations of the estimates $T_1,...,T_K$ for each data generating condition. We can calculate the following sample statistics for the estimates: 

- Sample mean: $\bar{T} = \frac{1}{K}\sum_{k=1}^K T_k$ 
- Sample variance: $S_T^2 = \frac{1}{K - 1}\sum_{k=1}^K \left(T_k - \bar{T}\right)^2$
- Sample skewness (standardized): $g_T = \frac{1}{K S_T^3}\sum_{k=1}^K \left(T_k - \bar{T}\right)^3$
- Sample kurtosis (standardized): $k_T = \frac{1}{K S_T^4} \sum_{k=1}^K \left(T_k - \bar{T}\right)^4$

The table below shows each of the performance criteria, what it measures, its definition, its estimate, and its MCSE formula. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(dplyr)
library(tibble)
library(kableExtra)

abs_dat <- tibble(Criterion = c("Bias","Variance","MSE", "RMSE"),
              Measure = c("Difference from true parameter", "Precision", "Accuracy", "Accuracy"),
              Definition = c("$\\text{E}(T) - \\theta$", "$\\text{E}\\left[(T - \\text{E}(T))^2\\right]$",  "$\\text{E}\\left[(T - \\theta)^2\\right]$", "$\\sqrt{\\text{E}\\left[(T - \\theta)^2\\right]}$"),
              Estimate = c("$\\bar{T} - \\theta$", "$S_T^2$", 
                           "$\\frac{1}{K}\\sum_{k=1}^{K}\\left(T_k - \\theta\\right)^2$", "$\\sqrt{\\frac{1}{K}\\sum_{k=1}^{K}\\left(T_k - \\theta\\right)^2}$"),
              MCSE = c("$\\sqrt{S_T^2/ K}$", "$S_T^2 \\sqrt{\\frac{k_T - 1}{K}}$",  
                       "$\\sqrt{\\frac{1}{K}\\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T(\\bar{T} - \\theta) + 4 S_T^2 (\\bar{T} - \\theta)^2\\right]}$ ", "$\\sqrt{\\frac{S_{MSE}^2}{4 MSE}}$ or $\\sqrt{\\frac{1}{K} \\left(RMSE_{(j)} - RMSE)^2\\right)}$"))

knitr::kable(abs_dat, escape = FALSE, caption = "Table 1. Absolute Performance Criteria") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
  
```

The equation for the MCSE of RMSE is derived using two methods. The first is [the delta method](https://www.jepusto.com/multivariate-delta-method/), which states that:

$$
\text{Var}\left(f(X)\right) = \left(\frac{\partial f}{\partial \theta}\right)^2 \text{Var}(X)
$$

Here let $X$ be $MSE$ and $f(X) = RMSE$ or $\sqrt{MSE}$

$$
\text{Var}\left(RMSE\right) = \left(\frac{1}{2\sqrt{MSE}}\right)^2 \text{Var}(MSE)
$$
$$
\text{Var}\left(RMSE\right) = \left(\frac{\text{Var}(MSE)}{4 \times MSE}\right) 
$$
$$
\text{MCSE}\left(RMSE\right) = \sqrt{\text{Var}(RMSE)}
$$


The second method is the jacknife technique, which involves excluding a replicate $j$ and calculating RMSE. The formula for RMSE is:

$$\sqrt{\frac{1}{K}\sum_{k=1}^{K}\left(T_k - \theta\right)^2}$$

This is approximately equivalent to:

$$RMSE = \sqrt{(\bar{T} - \theta)^2 + S^2_T}$$

The jacknife RMSE will be calculated as:

$$RMSE_{(j)}  = \sqrt{(\bar{T}_{(j)} - \theta)^2 + S^2_{T(j)}}$$

Here $\bar{T}_{(j)}$ $S^2_{T(j)}$ and indicate the mean and variance of the estimates leaving replicate $j$ out. Instead of each jacknife estimate, we use algebra tricks to calculate $\bar{T}_{(j)}$ and $S^2_{T(j)}$ as follows:

$$\bar{T}_{(j)} = \frac{1}{K} \left(K \bar{T} - T_j\right)$$


$$S_{T(j)}^2 = \frac{1}{K - 2} \left[(K - 1) S_T^2 - \frac{K}{K - 1}\left(T_j - \bar{T}\right)^2\right]$$

Then, the MCSE of $RMSE_{(j)}$ will be calculated as:

$$MCSE_{RMSE(JK)}  = \sqrt{\frac{1}{K} \left(RMSE_{(j)} - RMSE)^2\right)}$$

### Example

We use the `welch_res` dataset included in the package. It contains the results from an example simulation study comparing the heteroskedasticity robust Welch t-test to normal t-test. The code used to generate the data and results can be found [here](https://github.com/meghapsimatrix/SimHelpers/blob/master/data-raw/welch_res_dat.R). We varied the mean difference parameter when generating data for two groups. We set the values to 0, 0.5, 1 and 2. We also varied the sample size per group, setting sample size values to 50, 100, and 200. We generated the data with slightly unequal variances. With each simulated data, we ran normal t-test, which assumes homogeneity of variance, and we also ran Welch t-test which does not assume homogeneity of variance. We extracted the estimate (mean difference), variance of the estimates, p-value, and the upper and lower bounds of the confidence intervals. 

```{r, message = FALSE, warning = FALSE}
library(dplyr)
library(tibble)
library(SimHelpers)
library(knitr)
library(kableExtra)

welch_res %>%
  glimpse()
```


Below, we calculate the absolute performance criteria for the estimate of the mean difference. We present the results by sample size, mean difference, and the t-test method. The `calc_abs()` function is designed to work with the [`tidyeval`](https://tidyeval.tidyverse.org/index.html) workflow. The first argument, `res_dat`, requires a data frame or a tibble containing the results from a simulation study. The second argument, `estimates`, requires the name of the column containing the estimates like mean difference or regression coefficients. The third argument, `true_param`, requires the name of the column containing the true parameters. The fourth argument `perfm_criteria` lets the user specify which criteria to evaluate. The criteria can be specified using a character or character vector. If the user only wants bias, they can specify `perfm_criteria = "bias"`. If the user wants bias and root mean squared error, they can specify `perfm_criteria = c("bias", "rmse")`. By default, the function returns, bias, variance, mean squared error (mse), and root mean squared error (rmse). In the example below, we ask for all the available criteria. 

We use `dplyr` syntax to group by the sample size, mean difference used to generate the data, and the method used to analyze the data. We provide examples using [`do()`](https://dplyr.tidyverse.org/reference/do.html) and [`group_modify()`](https://dplyr.tidyverse.org/reference/group_map.html) to run `calc_abs()` on results for each combination of the conditions. We use [`mutate_if()`](https://dplyr.tidyverse.org/reference/mutate_all.html) to round the results to three decimal places. Note that `mutate_if()` ignores the grouping variables by default. 

```{r}
# using do()
welch_res %>%
  group_by(n, mean_diff, method) %>% # grouping 
  do(calc_abs(.,  estimates = est, true_param = mean_diff)) %>% # run the function
  mutate_if(is.numeric, round, 3) %>% # rounding
  kable() # create a kable table 
```


```{r}
# using group_modify()
welch_res %>%
  mutate(params = mean_diff) %>% # group_modify cannot take in a group column as an argument
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_abs(.x,  estimates = est, true_param = params)) %>%
  mutate_if(is.numeric, round, 3) %>%
  kable()
```


## Relative Criteria

Relative criteria can be useful to look at if multiple true parameter values are used to generate the data to run the simulation and if performance measure changes according to the true value. It can be only used when $|\theta| > 0$ as we cannot divide by $0$ (Morris et al., 2018). 

Table 2 below lists the criteria, definition, estimation and the MCSE formula. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
rel_dat <- tibble(Criterion = c("Relative Bias","Relative MSE"),
              Definition = c("$\\text{E}(L) / \\lambda$", "$\\text{E}\\left[(L - \\lambda)^2\\right]/ \\lambda^2$"),
              Estimate = c("$\\bar{L} / \\lambda$", "$\\frac{(\\bar{L} - \\lambda)^2 + S_L^2}{\\lambda^2}$"),
              MCSE = c("$\\sqrt{S_L^2 / (K\\lambda^2)}$", 
                       "$\\sqrt{\\frac{1}{K\\lambda^2}\\left[S_L^4 (k_L - 1) + 4 S_L^3 g_L(\\bar{L} - \\lambda) + 4 S_L^2 (\\bar{L} - \\lambda)^2\\right]}$"))

knitr::kable(rel_dat, escape = FALSE, caption = "Table 2. Relative Performance Criteria") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
  
```


### Example 

Below, we calculate the relative criteria for the mean difference estimates. Note that when mean difference is 0, the relative measures cannot be calculated. The function returns `NA` values for conditions where the mean difference is 0. The syntax for `calc_relative()` is similar to the one that we used earlier for `calc_abs()`. The `perfm_criteria` argument allows the user to specify which criteria to evaluate: `c("relative bias", "relative mse")`. 


```{r}
# using group_modify()
welch_res %>%
  mutate(params = mean_diff) %>%
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_relative(.x,  estimates = est, true_param = params)) %>%
  mutate_if(is.numeric, round, 3) %>%
  kable()
```

### Relative Bias of Variance Estimator

Variances estimators are always positive. Therefore, relative performance criteria are often used to assess variance estimators. Let $V$ denote the sampling variance of of a point-estimator $T$. To assess to relative bias of $V$, we need to divide it by the true value of the sampling variance of $T$ which we may not know. In such scenario, we can use the sample variance of $T$ across the replications ($S_T^2$) to estimate the true sampling variance $\text{Var}(T)$. The relative bias would then be estimated by $\bar{V} / S_T^2$, the average of the variance estimates divided by the sample variance of the point-estimates.

To estimate MCSE of the relative bias of the variance estimator, we need to account for the uncertainty in the estimation of the true sampling variance. One way to do so is to use the _jackknife_ technique. Jack-knifing involves excluding a replicate $j$ and calculating relative bias $\bar{V}_{(j)}/ S_{T(j)}^2$. Calculating all $K$  versions of this relative bias estimate, excluding each of replicates at a time, and taking the variance yields the jackknife variance estimator:

$$
MCSE\left(RB\right) = \frac{1}{K} \sum_{j=1}^K \left(\frac{\bar{V}_{(j)}}{S_{T(j)}^2} - \frac{\bar{V}}{S_T^2}\right)^2.
$$

To avoid the time consuming calculation to derive the MCSE as specified above, we reformulate the MCSE using some algebra tricks:

$$
\begin{aligned}
\bar{V}_{(j)} &= \frac{1}{K - 1}\left(K \bar{V} - V_j\right) \\
S_{T(j)}^2 &= \frac{1}{K - 2} \left[(K - 1) S_T^2 - \frac{K}{K - 1}\left(T_j - \bar{T}\right)^2\right]
\end{aligned}
$$

The function `calc_rbvjk()` calculates the jack-knife MCSE of the relative bias estimate of a variance estimator. The function requires `res_dat`, a data frame or tibble containing simulation results, `estimates`, the name of the column containing point-estimates, `var_estimates`, the name of the column containing the variance estimates, and `true_param`, the name of the column containing the true parameters of the point-estimator. 

```{r}
welch_res %>%
  mutate(params = mean_diff) %>%
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_rbvjk(.x,  estimates = est, var_estimates = var, true_param = params)) %>%
  mutate_if(is.numeric, round, 3) %>%
  kable()
```


## Hypothesis Testing and Confidence Intervals

When doing hypothesis tests, we are often interested in whether Type 1 error rate is controlled and whether the test has enough power to detect the effect of interest. Rejection rate captures the proportion of times the p-value is below a specified $\alpha$ level, the proportion of times we can reject the null hypothesis. When the specified effect is 0, we can examine Type 1 error rates and when the magnitude of the effect is greater than 0, we can examine power. We are also interested in confidence interval coverage, proportion of intervals that contain the true parameter, and the interval width, which is an indicator of the precision of the estimate.


```{r, echo = FALSE, warning = FALSE, message = FALSE}
hyp_dat <- tibble(Criterion = c("Rejection Rate","Coverage","Width"),
              Measure = c("Type 1 Error or Power", "Proportion of intervals containing true parameter", "Precision"),
              Definition = c("$\\rho_\\alpha = Pr(P_k) < \\alpha$", "$\\omega_\\beta = Pr(A \\leq \\theta \\leq B)$", "$\\text{E}(W) = \\text{E}(B - A)$"),
              Estimate = c("$r_\\alpha  = \\frac{1}{K} \\sum_{k=1}^K I(P_k < \\alpha)$", "$c_\\beta = \\frac{1}{K}\\sum_{k=1}^K I(A_k \\leq \\theta \\leq B_k)$", "$\\bar{W} = \\bar{B} - \\bar{A}$"),
              MCSE = c("$\\sqrt{r_\\alpha(1 - r_\\alpha) / K}$",  
                       "$\\sqrt{c_\\beta (1 - c_\\beta) / K}$", "$\\sqrt{S_W^2 / K}$"))

knitr::kable(hyp_dat, escape = FALSE, caption = "Table 3. Hypothesis Testing and Confidence Intervals Performance Criteria") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


### Example

Below we calculate the rejection rates for the hypothesis tests conducted using t-test. The null hypothesis states that the two groups have the same means. The `calc_rr()` function requires a data set or tibble as the first argument. The second argument, `p_values`, requires the name of the column containing p-values.  


```{r}
# using group_modify()
welch_res %>%
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_rr(.x, p_values = p_val)) %>%
  kable()
```

Below we calculate the confidence interval coverage rates and widths for the estimates of the mean difference. The `calc_coverage()` function requires a data frame or tibble containing the simulation results as the first argument. The second and third arguments, `lower_bound` and `upper_bound`, take in the name of the columns that contain the lower and upper bound estimates of the confidence interval. The `true_param` argument requires the name of the column containing the true parameters. Like `calc_abs()` and `calc_relative()`, `calc_coverage()` also has an argument `perfm_criteria` where the user can specify which criteria to evaluate: `c("coverage", "width")`. 

```{r}
# using group_modify()
welch_res %>%
  mutate(params = mean_diff) %>%
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_coverage(.x, lower_bound = lower_bound, upper_bound = upper_bound, true_param = params)) %>%
  kable()
```



# How to evaluate MCSE

The associated MCSE should be small compared to the performance measure. 

# References 

Morris, T. P., White, I. R., & Crowther, M. J. (2019). Using simulation studies to evaluate statistical methods. Statistics in medicine, 38(11), 2074-2102.

Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source
  Software, 4(43), 1686, https://doi.org/10.21105/joss.01686

