---
title: "Simulation Performance Criteria and MCSE"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{MCSE}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\VignetteDepends{dplyr}
  %\VignetteDepends{tibble}
  %\VignetteDepends{knitr}
  %\VignetteDepends{kableExtra}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Introduction to Simulation Studies and Monte Carlo Standard Errors

Simulations are experiments to study the performance of statistical methods under known data-generating conditions (Morris, White and Crowther, 2018). Methodologists examine questions like: (1) how does ordinary least squares (OLS) regression perform if errors are heteroskedastic? (2) how does the presence of missing data affect treatment effect estimates from a propensity score analysis? (3) how does robust variance estimation perform when sample size is small? To answer such questions, we conduct experiments by simulating thousands of datasets from pseudo-random sampling (Morris et al., 2018). We generate datasets under known conditions. For example, we can generate data where the errors are heteroskedastic or there is missing data present following Missing at Random (MAR) mechanism. Then, we apply statistical methods, like OLS and propensity score estimation using logistic regression, to each of the datasets and extract measures like regression coefficients, p-values, and confidence intervals. We then analyze the methods using performance criteria measures like bias, Type 1 error rate or root mean squared error. 

When examining performance measures, we need to also consider the error in the estimate of the performance measures due to the fact that we generate a finite number of samples. The error is quantified in the form of Monte Carlo standard error (MCSE). 

The goal of `simhelpers` is to assist in running simulation studies. This package provides a set of functions that calculates various performance measures and associated MCSE. This vignette focuses on the set of functions. Below, we explain three major categories of performance criteria: (1) absolute criteria, (2) relative criteria, and (3) criteria to evaluate hypothesis testing. We provide formulas used to calculate the performance measures and MCSE and demonstrate how to use the functions in the `simhelpers` package. 

The formula notations and explanations of the performance measures follow notes from Dr. James Pustejovsky's [Data Analysis, Simulation and Programming in R course (Spring, 2019)](https://www.jepusto.com/teaching/daspir/).


# Performance Criteria and MCSE

## Absolute Criteria

Bias characterizes whether the estimate on average lies above or below the true parameter. 
Variance characterizes the precision of the estimates. It characterizes the deviation of each estimate from the average of the estimates. Note that variance is the inverse of precision. Therefore, the higher the variance, the lower the precision. Mean squared error (MSE) and root mean squared error (RMSE) characterize the accuracy of the estimates. MSE and RMSE measure how far off on average the estimates are from the true parameter. Absolute criteria are in the same scale as the estimates. MSE is in squared deviation scale and RMSE is in the scale of the estimates.

Let $T$ denote an estimator for a parameter $\theta$. From running a simulation study, we obtain $K$ iterations of the estimates $T_1,...,T_K$ for each data generating condition. We can calculate the following sample statistics for the estimates: 

- Sample mean: $\bar{T} = \frac{1}{K}\sum_{k=1}^K T_k$ 
- Sample variance: $S_T^2 = \frac{1}{K - 1}\sum_{k=1}^K \left(T_k - \bar{T}\right)^2$
- Sample skewness (standardized): $g_T = \frac{1}{K S_T^3}\sum_{k=1}^K \left(T_k - \bar{T}\right)^3$
- Sample kurtosis (standardized): $k_T = \frac{1}{K S_T^4} \sum_{k=1}^K \left(T_k - \bar{T}\right)^4$

The table below shows each of the performance criteria, what it measures, its definition, its estimate, and its MCSE formula. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(dplyr)
library(tibble)
library(kableExtra)

abs_dat <- tibble(Criterion = c("Bias","Variance","MSE", "RMSE"),
              Measure = c("Difference from true parameter", "Precision", "Accuracy", "Accuracy"),
              Definition = c("$\\text{E}(T) - \\theta$", "$\\text{E}\\left[(T - \\text{E}(T))^2\\right]$",  "$\\text{E}\\left[(T - \\theta)^2\\right]$", "$\\sqrt{\\text{E}\\left[(T - \\theta)^2\\right]}$"),
              Estimate = c("$\\bar{T} - \\theta$", "$S_T^2$", 
                           "$\\frac{1}{K}\\sum_{k=1}^{K}\\left(T_k - \\theta\\right)^2$", "$\\sqrt{\\frac{1}{K}\\sum_{k=1}^{K}\\left(T_k - \\theta\\right)^2}$"),
              MCSE = c("$\\sqrt{S_T^2/ K}$", "$S_T^2 \\sqrt{\\frac{k_T - 1}{K}}$",  
                       "$\\sqrt{\\frac{1}{K}\\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T(\\bar{T} - \\theta) + 4 S_T^2 (\\bar{T} - \\theta)^2\\right]}$ ", "$\\sqrt{\\frac{1}{K} \\sum_{j=1}^K \\left(RMSE_{(j)} - RMSE)^2\\right)}$"))

knitr::kable(abs_dat, escape = FALSE, caption = "Table 1. Absolute Performance Criteria") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
  
```


The equation for the MCSE of RMSE is derived using the jackknife technique, which involves excluding a replicate $j$ and calculating RMSE. The formula for RMSE is:

$$\sqrt{\frac{1}{K}\sum_{k=1}^{K}\left(T_k - \theta\right)^2}$$

This is approximately equivalent to:

$$RMSE = \sqrt{(\bar{T} - \theta)^2 + S^2_T}$$

The jackknife RMSE will be calculated as:

$$RMSE_{(j)}  = \sqrt{(\bar{T}_{(j)} - \theta)^2 + S^2_{T(j)}}$$

Here $\bar{T}_{(j)}$ and $S^2_{T(j)}$ indicate the mean and variance of the estimates leaving replicate $j$ out. Instead of calculating each jackknife estimate, we use algebra tricks to calculate $\bar{T}_{(j)}$ and $S^2_{T(j)}$ as follows:

$$\bar{T}_{(j)} = \frac{1}{K-1} \left(K \bar{T} - T_j\right)$$


$$S_{T(j)}^2 = \frac{1}{K - 2} \left[(K - 1) S_T^2 - \frac{K}{K - 1}\left(T_j - \bar{T}\right)^2\right]$$

Then, the jacknife MCSE of RMSE will be calculated as:

$$MCSE_{RMSE(JK)}  = \sqrt{\frac{1}{K}\sum_{j=1}^K  \left(RMSE_{(j)} - RMSE\right)^2}$$

### Example

We use the `welch_res` dataset included in the package. It contains the results from an example simulation study comparing the heteroskedasticity robust Welch t-test to normal t-test. The code used to generate the data and derive results can be found [here](https://github.com/meghapsimatrix/simhelpers/blob/master/data-raw/welch_res_dat.R). We varied the mean difference parameter when generating data for two groups. We set the values to 0, 0.5, 1 and 2. We also varied the sample size per group, setting sample size values to 50, 100, and 200. We generated the data with slightly unequal variances. With each simulated data, we ran normal t-test, which assumes homogeneity of variance, and we also ran Welch t-test which does not assume homogeneity of variance. We extracted the estimate (mean difference), variance of the estimates, p-value, and the upper and lower bounds of the confidence intervals. 

```{r, message = FALSE, warning = FALSE}
library(simhelpers)
library(dplyr)
library(tibble)
library(knitr)
library(dplyr)
library(kableExtra)

welch_res %>%
  glimpse()
```


Below, we calculate the absolute performance criteria for the estimate of the mean difference. We present the results by sample size, mean difference, and the t-test method. The `calc_absolute()` function is designed to work with the [`tidyeval`](https://tidyeval.tidyverse.org/index.html) workflow. The first argument, `res_dat`, requires a data frame or a tibble containing the results from a simulation study. The second argument, `estimates`, requires the name of the column containing the estimates like mean difference or regression coefficients. The third argument, `true_param`, requires the name of the column containing the true parameters. The fourth argument `perfm_criteria` lets the user specify which criteria to evaluate. The criteria can be specified using a character or character vector. If the user only wants bias, they can specify `perfm_criteria = "bias"`. If the user wants bias and root mean squared error, they can specify `perfm_criteria = c("bias", "rmse")`. By default, the function returns, bias, variance, mean squared error (mse), and root mean squared error (rmse), `perfm_criteria = c("bias", "variance", "mse", "rmse")`. In the example below, we ask for all the available criteria. 

We use `dplyr` syntax to group by the sample size and mean difference that were used to generate the data, and the method used to analyze the data. We provide examples using [`do()`](https://dplyr.tidyverse.org/reference/do.html) and [`group_modify()`](https://dplyr.tidyverse.org/reference/group_map.html) to run `calc_absolute()` on results for each combination of the conditions. We round the results to five decimal places. 

```{r}
# using do()
welch_res %>%
  group_by(n, mean_diff, method) %>% # grouping 
  do(calc_absolute(.,  estimates = est, true_param = mean_diff)) %>% # run the function
  kable(digits = 5) # create a kable table 
```


```{r}
# using group_modify()
welch_res %>%
  mutate(params = mean_diff) %>% # group_modify cannot take in a group column as an argument
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_absolute(.x,  estimates = est, true_param = params)) %>%
  kable(digits = 5)
```


## Relative Criteria

Relative criteria can be useful to look at if multiple true parameter values are used to generate the data to run the simulation and if performance measure changes according to the true value. It can be only used when $|\theta| > 0$ as we cannot divide by $0$ (Morris et al., 2018). 

To derive the MCSE for relative RMSE, we again used the jackknife technique. The formula to calculate relative RMSE is:

$$RMSE = \sqrt{\frac{(\bar{T} - \theta)^2 + S_T^2}{\theta^2}}$$

The jackknife RMSE will be calculated as:

$$RMSE_{(j)}  = \sqrt{\frac{(\bar{T}_{(j)} - \theta)^2 + S_{T(j)}^2}{\theta^2}}$$

Here $\bar{T}_{(j)}$ and $S^2_{T(j)}$ are calculated as specified above when we described the algebra trick to estimate these values. The MCSE is then calculated as specified in the table below. 

Table 2 below shows each of the performance criteria, what it measures, its definition, its estimate, and its MCSE formula. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
rel_dat <- tibble(Criterion = c("Relative Bias","Relative MSE", "Relative RMSE"),
                  Measure = c("Relative difference from true parameter", "Accuracy", "Accuracy"),
              Definition = c("$\\text{E}(T) / \\theta$", "$\\text{E}\\left[(T - \\theta)^2\\right]/ \\theta^2$", "$\\sqrt{\\text{E}\\left[(T - \\theta)^2\\right]/ \\theta^2}$"),
              Estimate = c("$\\bar{T} / \\theta$", "$\\frac{(\\bar{T} - \\theta)^2 + S_T^2}{\\theta^2}$", "$\\sqrt{\\frac{(\\bar{T} - \\theta)^2 + S_T^2}{\\theta^2}}$"),
              MCSE = c("$\\sqrt{S_T^2 / (K\\theta^2)}$", 
                       "$\\sqrt{\\frac{1}{K\\theta^2}\\left[S_T^4 (k_T - 1) + 4 S_T^3 g_T(\\bar{T} - \\theta) + 4 S_T^2 (\\bar{T} - \\theta)^2\\right]}$", "$\\sqrt{\\frac{1}{K} \\sum_{j=1}^K \\left(RMSE_{(j)} - RMSE)^2\\right)}$"))

knitr::kable(rel_dat, escape = FALSE, caption = "Table 2. Relative Performance Criteria") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
  
```


### Example 

Below, we calculate the relative criteria for the mean difference estimates. Note that when mean difference is 0, the relative measures cannot be calculated. The function returns `NA` values for conditions where the mean difference is 0. The syntax for `calc_relative()` is similar to the one that we used earlier for `calc_absolute()`. The `perfm_criteria` argument allows the user to specify which criteria to evaluate: `perfm_criteria = c("relative bias", "relative mse", "relative rmse")`. 


```{r}
# using group_modify()
welch_res %>%
  mutate(params = mean_diff) %>%
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_relative(.x,  estimates = est, true_param = params)) %>%
  kable(digits = 5)
```

## Relative Criteria for Variance Estimators

Variances estimators are always positive. Therefore, relative performance criteria are often used to assess variance estimators. For variance estimators, we have $V$ denoting the sampling variance of of a point-estimator $T$. To assess the relative criteria for $V$, we need to divide it by the true value of the sampling variance of $T$, $\lambda = \text{Var}(T)$,  which we may not know. In such scenario, we can use the sample variance of $T$ across the replications, $S_T^2$, to estimate the true sampling variance. 

The relative bias would then be estimated by $RB = \bar{V} / S_T^2$, the average of the variance estimates divided by the sample variance of the point-estimates. To estimate MCSE of the relative bias of the variance estimator, we need to account for the uncertainty in the estimation of the true sampling variance. One way to do so is to use the _jackknife_ technique. Jack-knifing involves excluding a replicate $j$ and calculating relative bias $\bar{V}_{(j)}/ S_{T(j)}^2$. The Monte Carlo standard error can then be calculated as:

$$
MCSE\left(RB\right) = \sqrt{\frac{1}{K} \sum_{j=1}^K \left(RB_{(j)} - RB\right)^2}
$$
which can be written as:

$$
MCSE\left(RB\right) = \sqrt{\frac{1}{K} \sum_{j=1}^K \left(\frac{\bar{V}_{(j)}}{S_{T(j)}^2} - \frac{\bar{V}}{S_T^2}\right)^2}
$$


We reformulate the MCSE using some algebra tricks similar to how we reformulated them for the RMSE MCSE formulas above. 

$$
\begin{aligned}
\bar{V}_{(j)} &= \frac{1}{K - 1}\left(K \bar{V} - V_j\right) \\
S_{T(j)}^2 &= \frac{1}{K - 2} \left[(K - 1) S_T^2 - \frac{K}{K - 1}\left(T_j - \bar{T}\right)^2\right]
\end{aligned}
$$

Similarly, we can estimate the MCSE of relative MSE and RMSE using jackknife technique. To estimate the MSE we need to estimate $S_{V(j)}^2$ which represents the sample variance of the variance estimates leaving replicate $j$ out. We can estimate $S_{V(j)}^2$ as specified below: 

$$S_{V(j)}^2 = \frac{1}{K - 2} \left[(K - 1) S_V^2 - \frac{K}{K - 1}\left(V_j - \bar{V}\right)^2\right]$$

We can then estimate jackknife relative MSE and RMSE following the same logic as we described above and then calculate the MCSE. 

Table 3 below lists relative performance measures for variance estimators. 

```{r, echo = FALSE, warning = FALSE, message = FALSE}
rel_dat_var <- tibble(Criterion = c("Relative Bias","Relative MSE", "Relative RMSE"),
                      Measure = c("Relative difference from true parameter", "Accuracy", "Accuracy"),
              Definition = c("$\\text{E}(V) / \\lambda$", "$\\text{E}\\left[(V - \\lambda)^2\\right]/ \\lambda^2$", "$\\sqrt{\\text{E}\\left[(V - \\lambda)^2\\right]/ \\lambda^2}$"),
              Estimate = c("$\\bar{V} / S_T^2$", "$\\frac{(\\bar{V} - S_T^2)^2 + S_V^2}{S_T^4}$", "$\\sqrt{\\frac{(\\bar{V} - S_T^2)^2 + S_V^2}{S_T^4}}$"),
              MCSE = c("$\\sqrt{\\frac{1}{K} \\sum_{j=1}^K \\left(RB_{(j)} - RB\\right)^2}$", "$\\sqrt{\\frac{1}{K} \\sum_{j=1}^K \\left(MSE_{(j)} - MSE\\right)^2}$",
                      "$\\sqrt{\\frac{1}{K} \\sum_{j=1}^K \\left(RMSE_{(j)} - RMSE\\right)^2}$" ))


knitr::kable(rel_dat_var, escape = FALSE, caption = "Table 3. Relative Performance Criteria for Variance Estimators") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
  
```

The function `calc_relative_var()` calculates the performance criteria estimate and the jack-knife MCSE of the relative bias estimate of a variance estimator. The function requires `res_dat`, a data frame or tibble containing simulation results, `estimates`, the name of the column containing point-estimates, `var_estimates`, the name of the column containing the variance estimates, and `perfm_criteria`, the criteria to be evaluated: `perfm_criteria = c("relative bias", "relative mse", "relative rmse")`.

```{r}
welch_res %>%
  mutate(params = mean_diff) %>%
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_relative_var(.x,  estimates = est, var_estimates = var)) %>%
  kable(digits = 5)
```





## Hypothesis Testing and Confidence Intervals

When doing hypothesis tests, we are often interested in whether Type 1 error rate is adequately controlled and whether the test has enough power to detect the effect of interest. Rejection rate captures the proportion of times the p-value is below a specified $\alpha$ level, the proportion of times we can reject the null hypothesis. When the specified effect size is 0, we can examine Type 1 error rates and when the magnitude of the effect is greater than 0, we can examine power. We are also interested in confidence interval coverage, proportion of intervals that contain the true parameter, and the interval width, which is an indicator of the precision of the estimate.

Table 4 below presents the performance criteria used to evaluate hypothesis tests. In the table, let $P_k$ denote the p-value from simulation replication $k$, for $k = 1,...,K$. Suppose that the confidence intervals are for the target parameter $\theta$ and have coverage level $\beta$. Let $A_k$ and $B_k$ denote the lower and upper end-points of the confidence interval from simulation replication $k$, and let $W_k = B_k − A_k$, all for $k = 1,...,K$. 


```{r, echo = FALSE, warning = FALSE, message = FALSE}
hyp_dat <- tibble(Criterion = c("Rejection Rate","Coverage","Width"),
              Measure = c("Type 1 Error or Power", "Proportion of intervals containing true parameter", "Precision"),
              Definition = c("$\\rho_\\alpha = Pr(P_k) < \\alpha$", "$\\omega_\\beta = Pr(A \\leq \\theta \\leq B)$", "$\\text{E}(W) = \\text{E}(B - A)$"),
              Estimate = c("$r_\\alpha  = \\frac{1}{K} \\sum_{k=1}^K I(P_k < \\alpha)$", "$c_\\beta = \\frac{1}{K}\\sum_{k=1}^K I(A_k \\leq \\theta \\leq B_k)$", "$\\bar{W} = \\bar{B} - \\bar{A}$"),
              MCSE = c("$\\sqrt{r_\\alpha(1 - r_\\alpha) / K}$",  
                       "$\\sqrt{c_\\beta (1 - c_\\beta) / K}$", "$\\sqrt{S_W^2 / K}$"))

knitr::kable(hyp_dat, escape = FALSE, caption = "Table 4. Hypothesis Testing and Confidence Intervals Performance Criteria") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```


### Example

Below we calculate the rejection rates for the hypothesis tests conducted using t-test. The null hypothesis states that the two groups have the same means. The `calc_rejection()` function requires a data frame or tibble containing simulation results as the first argument. The second argument, `p_values`, requires the name of the column containing p-values. The third argument, `alpha` lets the user specify a value for $\alpha$. The default value is set to the conventional 0.05.   


```{r}
# using group_modify()
welch_res %>%
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_rejection(.x, p_values = p_val)) %>%
  kable(digits = 5)
```

Below we calculate the confidence interval coverage rates and widths for the estimates of the mean difference. The `calc_coverage()` function requires a data frame or tibble containing the simulation results as the first argument. The second and third arguments, `lower_bound` and `upper_bound`, take in the name of the columns that contain the lower and upper bound estimates of the confidence interval. The `true_param` argument requires the name of the column containing the true parameters. Like `calc_absolute()` and `calc_relative()`, `calc_coverage()` also has an argument, `perfm_criteria`, where the user can specify which criteria to evaluate: `perfm_criteria = c("coverage", "width")`. 

```{r}
# using group_modify()
welch_res %>%
  mutate(params = mean_diff) %>%
  group_by(n, mean_diff, method) %>%
  group_modify(~ calc_coverage(.x, lower_bound = lower_bound, upper_bound = upper_bound, true_param = params)) %>%
  kable(digits = 5)
```



# How to evaluate MCSE

The associated MCSE should be small compared to the performance measure. Below is an example from Frane (2015) of how to write up MCSE results as part of simulation study results. Frane (2015) compared various methods to control Type 1 error rates when conducting analysis on multiple outcome variables. 

"Only Bonferroni and MP strictly controlled the PFER; the observed maximum PFER was 0.050 for Bonferroni, 0.051 for Sidák, 0.061 for Holm, 0.100 for Hochberg, and 0.050 for MP (estimated SE ≤ 0.0002 for all estimates)."

The PFER refers to per family error rate, the expected number of Type 1 errors out of $m$ comparisons. MP refers to MANOVA protected tests. The SE reported in the bracket refers to the MCSE. The number of replications was 1,000,000 in Frane (2015). We note that the number is quite high and generally 1,000 to 10,000 replications are enough. The resulting MCSEs in Frane (2015) therefore are way smaller compared to the values for PFER. 

For more information for performance criteria and MCSE, we recommend Morris, White & Crowther (2018).

# References 

Frane, A. V. (2015). Power and Type I Error Control for Univariate Comparisons in Multivariate Two-Group Designs. Multivariate Behavioral Research, 50(2), 233–247. http://doi.org/10.1080/00273171.2014.968836

Morris, T. P., White, I. R., & Crowther, M. J. (2019). Using simulation studies to evaluate statistical methods. Statistics in medicine, 38(11), 2074-2102. http://doi.org/10.1002/sim.8086

Wickham et al., (2019). Welcome to the tidyverse. Journal of Open Source
  Software, 4(43), 1686, https://doi.org/10.21105/joss.01686

